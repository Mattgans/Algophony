{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u03a22-hQ8N"
      },
      "source": [
        "# Training with CVAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YJUYrV4PlN-E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# cuda setup\n",
        "device = torch.device(\"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "# 15000 images, 7 label class, 120 latent dimention, 100*250 pixels, latent size = 1000\n",
        "\n",
        "# hyper params\n",
        "batch_size = 36\n",
        "latent_size = 120 # latent dimension\n",
        "h_latent_size = 1000 # latent space size\n",
        "epochs = 30\n",
        "\n",
        "# original 400*1000\n",
        "\n",
        "height = 100 # of the image\n",
        "width = 250\n",
        "label_size = 7 # num of labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q0DMSqZHtMZs"
      },
      "outputs": [],
      "source": [
        "# implement multi label\n",
        "def multi_hot(labels, class_size):\n",
        "    multi_hot_labels = torch.zeros(len(labels), class_size)\n",
        "    for i, label_list in enumerate(labels):\n",
        "        for label in label_list:\n",
        "            multi_hot_labels[i, label] = 1\n",
        "    return multi_hot_labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIlS5Yc_OwVl",
        "outputId": "1cd64faa-2418-43ec-9b73-9884a8adaa0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "5411\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import ast\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# genre_map = {'70s': 0, '80s': 1, 'hiphop': 2, 'rock': 3, 'blues': 4, 'funk': 5, 'house': 6, 'soul': 7, 'pop': 8, 'electronic': 9,\n",
        "#              'jazz': 10, 'world': 11, 'ambient': 12, 'rnb': 13, 'experimental': 14, 'alt/indie': 15, 'country/folk': 16, 'classical': 17\n",
        "#              ,'symphony':17}\n",
        "\n",
        "\n",
        "instruments_map = {'drum': 0, 'bass': 1, 'guitar': 2, 'key': 3, 'vocal': 4, 'synth': 5, 'others': 6}\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.file = pd.read_csv(annotations_file)\n",
        "        counter = Counter(self.file['Instrument'])\n",
        "        print(counter['others'])\n",
        "\n",
        "        self.encoding_label = multi_hot([[instruments_map[i]] for i in self.file['Instrument']],label_size)\n",
        "\n",
        "        # self.img_labels = [ast.literal_eval(self.file['Instrument'][i]) for i in range(len(self.file))]\n",
        "        # self.encoding_label = multi_hot([[instruments_map[j] for j in i] for i in self.img_labels],label_size)\n",
        "\n",
        "        # returns a list of list of genres\n",
        "        # self.img_labels = [ast.literal_eval(self.file['Genre'][i]) for i in range(len(self.file))]\n",
        "        # encode with multi_hot\n",
        "        # self.encoding_label = multi_hot([[genre_map[j] for j in i] for i in self.img_labels],label_size)\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file['Instrument'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, f\"{str(idx+1)}.png\")\n",
        "        label = self.encoding_label[idx]\n",
        "        try:\n",
        "            image = Image.open(img_name).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening image {img_name}: {e}\")\n",
        "            image = torch.zeros(3,height,width)\n",
        "            return image,label\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((height, width)),  # Resize image to 1000x400\n",
        "    transforms.ToTensor(),           # Convert image to tensor\n",
        "])\n",
        "\n",
        "dataset = SpectrogramDataset(annotations_file='/content/drive/My Drive/new_data.csv',\n",
        "                        img_dir='/content/drive/My Drive/algophony_spectrograms/',transform = transform)\n",
        "\n",
        "#### image dimension 3 x 400 x 1000 ####\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quousJADkmlz"
      },
      "source": [
        "# Image Check\n",
        "Making sure the images are intact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJplzhmhb0bL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "for batch in dataloader:\n",
        "    images, labels = batch\n",
        "    # print(images.shape)\n",
        "    # print(labels.shape)\n",
        "    img_tensor = images[0]\n",
        "    # Convert tensor to PIL Image\n",
        "    img_pil = torchvision.transforms.functional.to_pil_image(img_tensor)\n",
        "    plt.imshow(img_pil)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4vtwqxdHcW3"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7vXAcGA8BXlT"
      },
      "outputs": [],
      "source": [
        "class CVAE(nn.Module):\n",
        "    def __init__(self, feature_size, latent_size, class_size):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.latent_size = latent_size\n",
        "        self.class_size = class_size\n",
        "\n",
        "        # encode\n",
        "        self.fc1  = nn.Linear(feature_size + class_size, h_latent_size)\n",
        "        self.fc21 = nn.Linear(h_latent_size, latent_size)\n",
        "        self.fc22 = nn.Linear(h_latent_size, latent_size)\n",
        "\n",
        "        # decode\n",
        "        self.fc3 = nn.Linear(latent_size + class_size, h_latent_size)\n",
        "        self.fc4 = nn.Linear(h_latent_size, feature_size)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def encode(self, x, c): # Q(z|x, c)\n",
        "        inputs = torch.cat([x, c], 1) # (bs, feature_size+class_size)\n",
        "        h1 = self.elu(self.fc1(inputs))\n",
        "        z_mu = self.fc21(h1)\n",
        "        z_var = self.fc22(h1)\n",
        "        return z_mu, z_var\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z, c): # P(x|z, c)\n",
        "        inputs = torch.cat([z, c], 1) # (bs, latent_size+class_size)\n",
        "        h3 = self.elu(self.fc3(inputs))\n",
        "        return self.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        mu, logvar = self.encode(x.view(-1, 3*width*height), c)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z, c), mu, logvar\n",
        "\n",
        "    def generate(self, c, num_samples=1):\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(num_samples, self.latent_size).to(c.device)\n",
        "            x_generated = self.decode(z, c)\n",
        "        return x_generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewgGH2XzIKyx"
      },
      "source": [
        "# Training Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8mfEfJ1tLqI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oCzvG1XchP55"
      },
      "outputs": [],
      "source": [
        "# create a CVAE model\n",
        "model = CVAE(3*width*height, latent_size, label_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # reconstruction error\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 3*width*height), reduction='sum')\n",
        "    # KL Divergence 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, labels) in enumerate(dataloader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        recon_batch, mu, logvar = model(data, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.detach().cpu().numpy()\n",
        "        optimizer.step()\n",
        "        # show progress\n",
        "        # if batch_idx % 200 == 0:\n",
        "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        #         epoch, batch_idx * len(data), len(dataloader.dataset),\n",
        "        #         100. * batch_idx / len(dataloader),\n",
        "        #         loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(dataloader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ep2WvnITdr"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK2WylyQCARl",
        "outputId": "4ed49760-38a1-4fec-801e-78f459dde465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error opening image /content/drive/My Drive/algophony_spectrograms/2965.png: [Errno 2] No such file or directory: '/content/drive/My Drive/algophony_spectrograms/2965.png'\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "    # test(epoch)\n",
        "    with torch.no_grad():\n",
        "      c = torch.eye(label_size, label_size).cpu()\n",
        "      sample = torch.randn(label_size, latent_size).to(device)\n",
        "      sample = model.decode(sample, c).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qCoM8lM4CBj-",
        "outputId": "da62fc2d-9b6a-443d-c8f4-8263ada12c26"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'multi_hot' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2e6f2699528b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Generate Hiphop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgenerated_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'multi_hot' is not defined"
          ]
        }
      ],
      "source": [
        "num_samples = 1  # Number of samples to generate\n",
        "\n",
        "# Generate Hiphop\n",
        "label = multi_hot(torch.tensor([[9]]),label_size)\n",
        "\n",
        "generated_samples = model.generate(label, num_samples=num_samples)\n",
        "\n",
        "# You can now use or visualize the generated samples as needed\n",
        "print(generated_samples.shape)\n",
        "\n",
        "img_tensor = generated_samples.reshape(3, height, width)\n",
        "img_pil = torchvision.transforms.functional.to_pil_image(img_tensor)\n",
        "plt.imshow(img_pil)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}